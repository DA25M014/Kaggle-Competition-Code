{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f32ae494",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T18:10:43.917145Z",
     "iopub.status.busy": "2025-11-19T18:10:43.916403Z",
     "iopub.status.idle": "2025-11-19T18:53:14.175937Z",
     "shell.execute_reply": "2025-11-19T18:53:14.174474Z"
    },
    "papermill": {
     "duration": 2550.268632,
     "end_time": "2025-11-19T18:53:14.180856",
     "exception": false,
     "start_time": "2025-11-19T18:10:43.912224",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Running on: cpu\n",
      " Loading JSON data...\n",
      " Loading metric names and embeddings...\n",
      " Building metric embedding matrices aligned with rows...\n",
      " Loading text (Gemma) embeddings...\n",
      " Performing data augmentation...\n",
      " Final training set shapes -> metrics: (20000, 768) | texts: (20000, 768) | labels: (20000,)\n",
      " Building feature matrices...\n",
      "X_train_full: (20000, 3073) | X_test_full: (3638, 3073)\n",
      " Starting hyperparameter search...\n",
      "\n",
      "====\n",
      " Trial 1/10\n",
      "Config: {'layers': [1024, 1024, 512, 128], 'dropout': 0.1, 'lr': 0.0001, 'batch_size': 128, 'wd': 1e-05, 'epochs': 15}\n",
      " Fold 1/3 for this trial\n",
      "    RMSE for this fold: 3.1462\n",
      " Fold 2/3 for this trial\n",
      "    RMSE for this fold: 3.1382\n",
      " Fold 3/3 for this trial\n",
      "    RMSE for this fold: 3.0989\n",
      " Trial RMSE: 3.1278\n",
      " New best config so far!\n",
      "\n",
      "====\n",
      " Trial 2/10\n",
      "Config: {'layers': [1024, 512, 128], 'dropout': 0.2, 'lr': 0.001, 'batch_size': 256, 'wd': 0.0001, 'epochs': 15}\n",
      " Fold 1/3 for this trial\n",
      "    RMSE for this fold: 2.9463\n",
      " Fold 2/3 for this trial\n",
      "    RMSE for this fold: 2.9680\n",
      " Fold 3/3 for this trial\n",
      "    RMSE for this fold: 2.9198\n",
      " Trial RMSE: 2.9447\n",
      " New best config so far!\n",
      "\n",
      "====\n",
      " Trial 3/10\n",
      "Config: {'layers': [1024, 512, 128], 'dropout': 0.1, 'lr': 0.001, 'batch_size': 256, 'wd': 1e-05, 'epochs': 15}\n",
      " Fold 1/3 for this trial\n",
      "    RMSE for this fold: 2.9621\n",
      " Fold 2/3 for this trial\n",
      "    RMSE for this fold: 2.9863\n",
      " Fold 3/3 for this trial\n",
      "    RMSE for this fold: 2.8896\n",
      " Trial RMSE: 2.9460\n",
      "\n",
      "====\n",
      " Trial 4/10\n",
      "Config: {'layers': [1024, 1024, 512, 128], 'dropout': 0.2, 'lr': 0.0001, 'batch_size': 256, 'wd': 0.0001, 'epochs': 15}\n",
      " Fold 1/3 for this trial\n",
      "    RMSE for this fold: 3.2696\n",
      " Fold 2/3 for this trial\n",
      "    RMSE for this fold: 3.2805\n",
      " Fold 3/3 for this trial\n",
      "    RMSE for this fold: 3.2659\n",
      " Trial RMSE: 3.2720\n",
      "\n",
      "====\n",
      " Trial 5/10\n",
      "Config: {'layers': [1024, 512, 128], 'dropout': 0.3, 'lr': 0.0001, 'batch_size': 256, 'wd': 1e-05, 'epochs': 15}\n",
      " Fold 1/3 for this trial\n",
      "    RMSE for this fold: 3.2775\n",
      " Fold 2/3 for this trial\n",
      "    RMSE for this fold: 3.3410\n",
      " Fold 3/3 for this trial\n",
      "    RMSE for this fold: 3.2115\n",
      " Trial RMSE: 3.2767\n",
      "\n",
      "====\n",
      " Trial 6/10\n",
      "Config: {'layers': [1024, 512, 128], 'dropout': 0.3, 'lr': 0.0005, 'batch_size': 256, 'wd': 1e-05, 'epochs': 15}\n",
      " Fold 1/3 for this trial\n",
      "    RMSE for this fold: 2.8946\n",
      " Fold 2/3 for this trial\n",
      "    RMSE for this fold: 2.9971\n",
      " Fold 3/3 for this trial\n",
      "    RMSE for this fold: 2.9076\n",
      " Trial RMSE: 2.9331\n",
      " New best config so far!\n",
      "\n",
      "====\n",
      " Trial 7/10\n",
      "Config: {'layers': [1024, 512, 128], 'dropout': 0.3, 'lr': 0.0005, 'batch_size': 128, 'wd': 0.0001, 'epochs': 15}\n",
      " Fold 1/3 for this trial\n",
      "    RMSE for this fold: 2.8906\n",
      " Fold 2/3 for this trial\n",
      "    RMSE for this fold: 2.9617\n",
      " Fold 3/3 for this trial\n",
      "    RMSE for this fold: 2.8767\n",
      " Trial RMSE: 2.9096\n",
      " New best config so far!\n",
      "\n",
      "====\n",
      " Trial 8/10\n",
      "Config: {'layers': [1024, 512, 128], 'dropout': 0.2, 'lr': 0.0005, 'batch_size': 128, 'wd': 1e-05, 'epochs': 15}\n",
      " Fold 1/3 for this trial\n",
      "    RMSE for this fold: 2.9234\n",
      " Fold 2/3 for this trial\n",
      "    RMSE for this fold: 2.9238\n",
      " Fold 3/3 for this trial\n",
      "    RMSE for this fold: 2.8756\n",
      " Trial RMSE: 2.9076\n",
      " New best config so far!\n",
      "\n",
      "====\n",
      " Trial 9/10\n",
      "Config: {'layers': [1024, 512, 128], 'dropout': 0.2, 'lr': 0.001, 'batch_size': 128, 'wd': 1e-05, 'epochs': 15}\n",
      " Fold 1/3 for this trial\n",
      "    RMSE for this fold: 2.8756\n",
      " Fold 2/3 for this trial\n",
      "    RMSE for this fold: 2.8599\n",
      " Fold 3/3 for this trial\n",
      "    RMSE for this fold: 2.8283\n",
      " Trial RMSE: 2.8546\n",
      " New best config so far!\n",
      "\n",
      "====\n",
      " Trial 10/10\n",
      "Config: {'layers': [1024, 1024, 512, 128], 'dropout': 0.2, 'lr': 0.001, 'batch_size': 256, 'wd': 0.0001, 'epochs': 15}\n",
      " Fold 1/3 for this trial\n",
      "    RMSE for this fold: 2.9894\n",
      " Fold 2/3 for this trial\n",
      "    RMSE for this fold: 3.0097\n",
      " Fold 3/3 for this trial\n",
      "    RMSE for this fold: 2.9964\n",
      " Trial RMSE: 2.9985\n",
      "\n",
      " Hyperparameter search finished.\n",
      "Best RMSE: 2.854623\n",
      "Best Config: {'layers': [1024, 512, 128], 'dropout': 0.2, 'lr': 0.001, 'batch_size': 128, 'wd': 1e-05, 'epochs': 15}\n",
      "\n",
      " Final training with best config...\n",
      "\n",
      " Final Fold 1/5\n",
      "\n",
      " Final Fold 2/5\n",
      "\n",
      " Final Fold 3/5\n",
      "\n",
      " Final Fold 4/5\n",
      "\n",
      " Final Fold 5/5\n",
      "\n",
      " Calibrating with simple linear regression on OOF...\n",
      "\n",
      " Saved submission.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\" Running on:\", DEVICE)\n",
    "\n",
    "TRAIN_PATH = \"/kaggle/input/da5401-2025-data-challenge/train_data.json\"\n",
    "TEST_PATH = \"/kaggle/input/da5401-2025-data-challenge/test_data.json\"\n",
    "METRIC_PATH = \"/kaggle/input/da5401-2025-data-challenge/metric_names.json\"\n",
    "METRIC_EMB_PATH = \"/kaggle/input/da5401-2025-data-challenge/metric_name_embeddings.npy\"\n",
    "\n",
    "GEMMA_TRAIN_EMB = \"/kaggle/input/embeddings-1/gemma_train_emb.npy\"\n",
    "GEMMA_TEST_EMB = \"/kaggle/input/embeddings-1/gemma_test_emb.npy\"\n",
    "\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "print(\" Loading JSON data...\")\n",
    "with open(TRAIN_PATH, \"r\") as f:\n",
    "    train_json = json.load(f)\n",
    "with open(TEST_PATH, \"r\") as f:\n",
    "    test_json = json.load(f)\n",
    "\n",
    "print(\" Loading metric names and embeddings...\")\n",
    "with open(METRIC_PATH, \"r\") as f:\n",
    "    metric_names_raw = json.load(f)\n",
    "metric_name_emb_table = np.load(METRIC_EMB_PATH)\n",
    "\n",
    "if isinstance(metric_names_raw, list):\n",
    "    if isinstance(metric_names_raw[0], str):\n",
    "        metric_name_to_idx = {name: i for i, name in enumerate(metric_names_raw)}\n",
    "    elif isinstance(metric_names_raw[0], dict):\n",
    "        if \"metric_name\" in metric_names_raw[0]:\n",
    "            metric_name_to_idx = {\n",
    "                d[\"metric_name\"]: i for i, d in enumerate(metric_names_raw)\n",
    "            }\n",
    "        elif \"name\" in metric_names_raw[0]:\n",
    "            metric_name_to_idx = {d[\"name\"]: i for i, d in enumerate(metric_names_raw)}\n",
    "        else:\n",
    "            raise ValueError(\"Do not know how to read metric_names.json dict format.\")\n",
    "else:\n",
    "    raise ValueError(\"Unexpected format in metric_names.json\")\n",
    "\n",
    "sample_train_row = train_json[0]\n",
    "if \"metric_name\" in sample_train_row:\n",
    "    metric_field = \"metric_name\"\n",
    "elif \"metric\" in sample_train_row:\n",
    "    metric_field = \"metric\"\n",
    "else:\n",
    "    raise ValueError(\"Train JSON rows do not have 'metric_name' or 'metric' key.\")\n",
    "\n",
    "print(\" Building metric embedding matrices aligned with rows...\")\n",
    "train_metric_embs = np.stack(\n",
    "    [metric_name_emb_table[metric_name_to_idx[row[metric_field]]] for row in train_json]\n",
    ").astype(np.float32)\n",
    "test_metric_embs = np.stack(\n",
    "    [metric_name_emb_table[metric_name_to_idx[row[metric_field]]] for row in test_json]\n",
    ").astype(np.float32)\n",
    "\n",
    "print(\" Loading text (Gemma) embeddings...\")\n",
    "train_text_embs = np.load(GEMMA_TRAIN_EMB).astype(np.float32)\n",
    "test_text_embs = np.load(GEMMA_TEST_EMB).astype(np.float32)\n",
    "\n",
    "if len(train_metric_embs) != len(train_text_embs):\n",
    "    raise ValueError(\n",
    "        f\"Train metric emb len {len(train_metric_embs)} \"\n",
    "        f\"!= train text emb len {len(train_text_embs)}\"\n",
    "    )\n",
    "\n",
    "y_train = pd.DataFrame(train_json)[\"score\"].values.astype(np.float32)\n",
    "\n",
    "print(\" Performing data augmentation...\")\n",
    "rng = np.random.default_rng(42)\n",
    "N = len(train_metric_embs)\n",
    "\n",
    "perm = rng.permutation(N)\n",
    "neg_m1, neg_t1 = train_metric_embs, train_text_embs[perm]\n",
    "\n",
    "noise = rng.normal(scale=0.6, size=train_text_embs.shape).astype(np.float32)\n",
    "neg_m2, neg_t2 = train_metric_embs, train_text_embs + noise\n",
    "\n",
    "perm2 = rng.permutation(N)\n",
    "neg_m3, neg_t3 = train_metric_embs[perm2], train_text_embs\n",
    "\n",
    "neg_y = rng.integers(0, 4, size=N * 3).astype(np.float32)\n",
    "\n",
    "m_all = np.vstack([train_metric_embs, neg_m1, neg_m2, neg_m3])\n",
    "t_all = np.vstack([train_text_embs,   neg_t1, neg_t2, neg_t3])\n",
    "y_all = np.concatenate([y_train,      neg_y])\n",
    "\n",
    "print(\" Final training set shapes ->\",\n",
    "      \"metrics:\", m_all.shape,\n",
    "      \"| texts:\", t_all.shape,\n",
    "      \"| labels:\", y_all.shape)\n",
    "\n",
    "\n",
    "def build_features(metric_emb, text_emb):\n",
    "    abs_diff = np.abs(metric_emb - text_emb)\n",
    "    prod = metric_emb * text_emb\n",
    "    m_norm = metric_emb / (np.linalg.norm(metric_emb, axis=1, keepdims=True) + 1e-9)\n",
    "    t_norm = text_emb / (np.linalg.norm(text_emb, axis=1, keepdims=True) + 1e-9)\n",
    "    cosine = np.sum(m_norm * t_norm, axis=1, keepdims=True)\n",
    "    return np.hstack([metric_emb, text_emb, abs_diff, prod, cosine]).astype(np.float32)\n",
    "\n",
    "\n",
    "print(\" Building feature matrices...\")\n",
    "X_train_full = build_features(m_all, t_all)\n",
    "X_test_full = build_features(test_metric_embs, test_text_embs)\n",
    "\n",
    "print(\"X_train_full:\", X_train_full.shape, \"| X_test_full:\", X_test_full.shape)\n",
    "\n",
    "\n",
    "class DynamicMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layers=[1024, 512, 128], dropout=0.2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        curr = input_dim\n",
    "        for h in hidden_layers:\n",
    "            layers.append(nn.Linear(curr, h))\n",
    "            layers.append(nn.BatchNorm1d(h))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            curr = h\n",
    "        layers.append(nn.Linear(curr, 1))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1)\n",
    "\n",
    "\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X, self.y = X, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i]\n",
    "\n",
    "\n",
    "def run_training(config, X, y, n_folds=3):\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "    for fold, (tr, va) in enumerate(kf.split(X)):\n",
    "        print(f\" Fold {fold+1}/{n_folds} for this trial\")\n",
    "        X_tr, y_tr = X[tr], y[tr]\n",
    "        X_val, y_val = X[va], y[va]\n",
    "\n",
    "        train_dl = DataLoader(\n",
    "            TabularDataset(X_tr, y_tr),\n",
    "            batch_size=config[\"batch_size\"],\n",
    "            shuffle=True,\n",
    "        )\n",
    "        val_dl = DataLoader(\n",
    "            TabularDataset(X_val, y_val),\n",
    "            batch_size=config[\"batch_size\"],\n",
    "            shuffle=False,\n",
    "        )\n",
    "\n",
    "        model = DynamicMLP(X.shape[1], config[\"layers\"], config[\"dropout\"]).to(DEVICE)\n",
    "        opt = optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=config[\"lr\"],\n",
    "            weight_decay=config[\"wd\"],\n",
    "        )\n",
    "        sch = optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"min\", factor=0.5, patience=2)\n",
    "        crit = nn.MSELoss()\n",
    "\n",
    "        best = float(\"inf\")\n",
    "        patience = 0\n",
    "\n",
    "        for epoch in range(config[\"epochs\"]):\n",
    "            model.train()\n",
    "            for xb, yb in train_dl:\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                opt.zero_grad()\n",
    "                loss = crit(model(xb), yb)\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "\n",
    "            model.eval()\n",
    "            preds = []\n",
    "            with torch.no_grad():\n",
    "                for xb, yb in val_dl:\n",
    "                    preds.append(model(xb.to(DEVICE)).cpu().numpy())\n",
    "            rmse = np.sqrt(mean_squared_error(y_val, np.concatenate(preds)))\n",
    "            sch.step(rmse)\n",
    "\n",
    "            if rmse < best:\n",
    "                best = rmse\n",
    "                patience = 0\n",
    "            else:\n",
    "                patience += 1\n",
    "                if patience >= 4:\n",
    "                    break\n",
    "\n",
    "        scores.append(best)\n",
    "        print(f\"    RMSE for this fold: {best:.4f}\")\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "print(\" Starting hyperparameter search...\")\n",
    "\n",
    "param_grid = {\n",
    "    \"layers\": [\n",
    "        [1024, 512, 128],\n",
    "        [2048, 1024, 256],\n",
    "        [1024, 1024, 512, 128],\n",
    "    ],\n",
    "    \"dropout\": [0.1, 0.2, 0.3],\n",
    "    \"lr\": [1e-3, 5e-4, 1e-4],\n",
    "    \"batch_size\": [128, 256],\n",
    "    \"wd\": [1e-4, 1e-5],\n",
    "    \"epochs\": [15],\n",
    "}\n",
    "\n",
    "keys, values = zip(*param_grid.items())\n",
    "all_trials = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "trials = random.sample(all_trials, k=min(10, len(all_trials)))\n",
    "\n",
    "best_score = float(\"inf\")\n",
    "best_config = None\n",
    "\n",
    "for i, cfg in enumerate(trials):\n",
    "    print(f\"\\n====\")\n",
    "    print(f\" Trial {i+1}/{len(trials)}\")\n",
    "    print(\"Config:\", cfg)\n",
    "    try:\n",
    "        score = run_training(cfg, X_train_full, y_all)\n",
    "        print(f\" Trial RMSE: {score:.4f}\")\n",
    "        if score < best_score:\n",
    "            best_score = score\n",
    "            best_config = cfg\n",
    "            print(\" New best config so far!\")\n",
    "    except Exception as e:\n",
    "        print(\" Trial failed with error:\", e)\n",
    "\n",
    "print(\"\\n Hyperparameter search finished.\")\n",
    "print(\"Best RMSE:\", best_score)\n",
    "print(\"Best Config:\", best_config)\n",
    "\n",
    "print(\"\\n Final training with best config...\")\n",
    "best_config[\"epochs\"] = 25\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "test_preds_accum = np.zeros((5, len(X_test_full)))\n",
    "oof = np.zeros(len(X_train_full))\n",
    "\n",
    "for fold, (tr, va) in enumerate(kf.split(X_train_full)):\n",
    "    print(f\"\\n Final Fold {fold+1}/5\")\n",
    "    X_tr, y_tr = X_train_full[tr], y_all[tr]\n",
    "    X_val, y_val = X_train_full[va], y_all[va]\n",
    "\n",
    "    train_dl = DataLoader(\n",
    "        TabularDataset(X_tr, y_tr),\n",
    "        batch_size=best_config[\"batch_size\"],\n",
    "        shuffle=True,\n",
    "    )\n",
    "    val_dl = DataLoader(\n",
    "        TabularDataset(X_val, y_val),\n",
    "        batch_size=best_config[\"batch_size\"],\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    model = DynamicMLP(X_train_full.shape[1], best_config[\"layers\"], best_config[\"dropout\"]).to(DEVICE)\n",
    "    opt = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=best_config[\"lr\"],\n",
    "        weight_decay=best_config[\"wd\"],\n",
    "    )\n",
    "    sch = optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"min\", factor=0.5, patience=3)\n",
    "    crit = nn.MSELoss()\n",
    "    best_rmse = float(\"inf\")\n",
    "\n",
    "    for epoch in enumerate(range(best_config[\"epochs\"])):\n",
    "        model.train()\n",
    "        for xb, yb in train_dl:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            opt.zero_grad()\n",
    "            loss = crit(model(xb), yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        model.eval()\n",
    "        preds = []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_dl:\n",
    "                preds.append(model(xb.to(DEVICE)).cpu().numpy())\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, np.concatenate(preds)))\n",
    "        sch.step(rmse)\n",
    "\n",
    "        if rmse < best_rmse:\n",
    "            best_rmse = rmse\n",
    "            torch.save(model.state_dict(), f\"final_model_fold{fold}.pt\")\n",
    "\n",
    "    model.load_state_dict(torch.load(f\"final_model_fold{fold}.pt\"))\n",
    "    model.eval()\n",
    "\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_dl:\n",
    "            preds.append(model(xb.to(DEVICE)).cpu().numpy())\n",
    "    oof[va] = np.concatenate(preds)\n",
    "\n",
    "    tpreds = []\n",
    "    test_dl = DataLoader(torch.tensor(X_test_full), batch_size=best_config[\"batch_size\"], shuffle=False)\n",
    "    with torch.no_grad():\n",
    "        for xb in test_dl:\n",
    "            tpreds.append(model(xb.to(DEVICE)).cpu().numpy())\n",
    "    test_preds_accum[fold] = np.concatenate(tpreds)\n",
    "\n",
    "print(\"\\n Calibrating with simple linear regression on OOF...\")\n",
    "cal = LinearRegression()\n",
    "cal.fit(oof.reshape(-1, 1), y_all)\n",
    "\n",
    "avg = test_preds_accum.mean(axis=0)\n",
    "final = cal.predict(avg.reshape(-1, 1))\n",
    "final = np.clip(final, 0, 10)\n",
    "\n",
    "ids = np.arange(1, len(final) + 1)\n",
    "sub = pd.DataFrame({\"ID\": ids, \"score\": final})\n",
    "sub.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "print(\"\\n Saved submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 14294892,
     "sourceId": 118082,
     "sourceType": "competition"
    },
    {
     "datasetId": 8782872,
     "sourceId": 13795148,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2558.891899,
   "end_time": "2025-11-19T18:53:17.128874",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-19T18:10:38.236975",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
